import asr
import asyncio
from analizers.mistake_analyzer import MistakeAnalyzer
from analizers.qa_evaluator import QAEvaluator
import json
from pathlib import Path
from typing import Optional
from config import load_config

def _find_cached_transcript_json(file_path: str, output_dir: Optional[str]) -> Optional[str]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç.

    –ò—â–µ–º –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ:
    1) <output_dir>/<stem>/<stem>_transcript.json
    2) results/<stem>/<stem>_transcript.json
    3) –†—è–¥–æ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–æ–º: *_transcript.json (legacy)
    """
    p = Path(file_path)
    stem = p.stem
    candidates = []
    if output_dir:
        candidates.append(str(Path(output_dir) / stem / f"{stem}_transcript.json"))
    else:
        base = Path.cwd() / "results" / stem
        candidates.append(str(base / f"{stem}_transcript.json"))
    # legacy —Ä—è–¥–æ–º —Å —Ñ–∞–π–ª–æ–º
    candidates.append(str(p.with_suffix('')) + "_transcript.json")
    for c in candidates:
        if Path(c).is_file():
            return c
    return None

def _prepare_output_dir(file_path: str, output_dir: Optional[str]) -> Path:
    p = Path(file_path)
    stem = p.stem
    base_dir = Path(output_dir) if output_dir else (Path.cwd() / "results")
    out_dir = base_dir / stem
    out_dir.mkdir(parents=True, exist_ok=True)
    return out_dir

def _load_segments_from_json(json_path: str):
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, dict):
            # –ù–∞—à —Ñ–æ—Ä–º–∞—Ç: transcript, –≤–æ–∑–º–æ–∂–Ω—ã –≤–∞—Ä–∏–∞–Ω—Ç—ã: segments
            segments = data.get("transcript") or data.get("segments")
            # –ò–Ω–æ–≥–¥–∞ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–ª–æ–∂–µ–Ω–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if segments is None and "result" in data and isinstance(data["result"], dict):
                segments = data["result"].get("segments")
        elif isinstance(data, list):
            segments = data
        else:
            segments = None
        return segments
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∏–∑ {json_path}: {e}")
        return None


async def analyze_interview_file(file_path: str, segments=None, output_dir: Optional[str] = None):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –∏–ª–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç.

    - –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã `segments`, –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é.
    - –ï—Å–ª–∏ `file_path` —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ .json –∏–ª–∏ —Ä—è–¥–æ–º/–≤ results –µ—Å—Ç—å *_transcript.json —Å –ø–æ–ª–µ–º transcript, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –µ–≥–æ.
    - –ò–Ω–∞—á–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç WhisperX —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—é.
    """
    print(f"üîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –≤—Ö–æ–¥–∞: {file_path}")

    # 0. –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç
    if segments is not None:
        print("üìù –ò—Å–ø–æ–ª—å–∑—É—é —É–∂–µ –≥–æ—Ç–æ–≤—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç (–ø–µ—Ä–µ–¥–∞–Ω –≤ —Ñ—É–Ω–∫—Ü–∏—é)")
    else:
        p = Path(file_path)
        used_cached = False
        if p.suffix.lower() == ".json":
            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞–ª json —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–º
            print("üìù –û–±–Ω–∞—Ä—É–∂–µ–Ω JSON. –ü—ã—Ç–∞—é—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç...")
            segments = _load_segments_from_json(str(p))
            used_cached = segments is not None
        else:
            # –ò—â–µ–º —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç (legacy –∏ –Ω–æ–≤–∞—è —Å—Ö–µ–º–∞ –ø–∞–ø–æ–∫)
            candidate = _find_cached_transcript_json(str(p), output_dir)
            if candidate and Path(candidate).is_file():
                print(f"üìù –ù–∞–π–¥–µ–Ω —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: {candidate}")
                segments = _load_segments_from_json(candidate)
                used_cached = segments is not None

        if used_cached:
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞")
        else:
            # 1. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ–º –∞—É–¥–∏–æ
            print("üìù –ì–æ—Ç–æ–≤–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ –Ω–µ—Ç. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É—é –∞—É–¥–∏–æ...")
            result = await asr.transcribe(file_path)
            if not result or "segments" not in result:
                print("‚ùå –û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é")
                return
            segments = result["segments"]
            print(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –ø—Ä–∏ —Å–±–æ–µ LLM
            out_dir_early = _prepare_output_dir(file_path, output_dir)
            stem = Path(file_path).stem
            transcript_file = out_dir_early / f"{stem}_transcript.json"
            try:
                with open(transcript_file, 'w', encoding='utf-8') as tf:
                    json.dump({"segments": segments}, tf, ensure_ascii=False, indent=2)
                print(f"üíæ –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {transcript_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç: {e}")
    
    # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    print("üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –æ—à–∏–±–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞...")
    # –ó–∞–≥—Ä—É–∑–∏–º –∫–æ–Ω—Ñ–∏–≥, —á—Ç–æ–±—ã –ø—Ä–æ–∫–∏–Ω—É—Ç—å –º–æ–¥–µ–ª–∏ –≤ –∞–Ω–∞–ª–∞–π–∑–µ—Ä—ã
    cfg = load_config(None)
    models_cfg = cfg.get("models", {}) if isinstance(cfg, dict) else {}
    mistakes_model = models_cfg.get("mistakes") or "gemini-2.0-flash"
    qa_model = models_cfg.get("qa") or "gemini-2.0-flash"

    # –ß–∞–Ω–∫-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –≤—ã–∫–ª—é—á–µ–Ω—ã)
    chunk_cfg = cfg.get("chunking", {}) if isinstance(cfg, dict) else {}
    m_chunk = (chunk_cfg.get("mistakes") or {}) if isinstance(chunk_cfg, dict) else {}
    q_chunk = (chunk_cfg.get("qa") or {}) if isinstance(chunk_cfg, dict) else {}

    m_enabled = bool(m_chunk.get("enabled", False))
    m_chunk_sec = int(m_chunk.get("chunk_duration_sec", 0) or 0) if m_enabled else None
    m_overlap = int(m_chunk.get("overlap_sec", 15) or 15)
    m_max_chunks = m_chunk.get("max_chunks")
    m_max_hint = m_chunk.get("max_issues_hint")

    mistakes_analyzer = MistakeAnalyzer(
        model_name=mistakes_model,
        chunk_duration_sec=m_chunk_sec,
        overlap_sec=m_overlap,
        max_chunks=m_max_chunks,
        max_issues_hint=m_max_hint,
    )
    mistakes_analysis = mistakes_analyzer.analyze_mistakes(segments)

    # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç—á–µ—Ç –ø–æ –æ—à–∏–±–∫–∞–º
    print("üìä –ì–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç—á–µ—Ç –ø–æ –æ—à–∏–±–∫–∞–º...")
    mistakes_report = mistakes_analyzer.generate_report(mistakes_analysis)

    # 4. –û—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞ (QA)
    print("üß™ –û—Ü–µ–Ω–∏–≤–∞—é –æ—Ç–≤–µ—Ç—ã –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –∏–Ω—Ç–µ—Ä–≤—å—é–µ—Ä–∞...")
    q_enabled = bool(q_chunk.get("enabled", False))
    q_chunk_sec = int(q_chunk.get("chunk_duration_sec", 0) or 0) if q_enabled else None
    q_overlap = int(q_chunk.get("overlap_sec", 15) or 15)
    q_max_chunks = q_chunk.get("max_chunks")
    q_max_hint = q_chunk.get("max_items_hint")

    qa = QAEvaluator(
        model_name=qa_model,
        chunk_duration_sec=q_chunk_sec,
        overlap_sec=q_overlap,
        max_chunks=q_max_chunks,
        max_items_hint=q_max_hint,
    )
    candidate_speaker = (mistakes_analysis.assumptions or {}).get("candidate_speaker")
    qa_eval = qa.analyze_qa(segments, candidate_speaker=candidate_speaker)
    qa_report = qa.generate_report(qa_eval)

    # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–æ—à–∏–±–∫–∏ + QA) –≤ –ø–∞–ø–∫—É –¥–ª—è —Ñ–∞–π–ª–∞
    out_dir = _prepare_output_dir(file_path, output_dir)
    stem = Path(file_path).stem

    mistakes_output_file = out_dir / f"{stem}_mistakes.md"
    with open(mistakes_output_file, 'w', encoding='utf-8') as f:
        f.write(mistakes_report)

    qa_output_file = out_dir / f"{stem}_qa_review.md"
    with open(qa_output_file, 'w', encoding='utf-8') as f:
        f.write(qa_report)

    # 5.1 Study guide (Hotspots + –ü–ª–∞–Ω –ø—Ä–æ–∫–∞—á–∫–∏) –∏ CSV —Å Q&A
    def _build_study_guide(low_threshold: float = 6.0, max_qa_hotspots: int = 20) -> str:
        guide = ["# –ü–ª–∞–Ω –ø—Ä–æ–∫–∞—á–∫–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è\n"]

        # Hotspots: —Ö—É–¥—à–∏–µ –æ—Ç–≤–µ—Ç—ã –∏ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
        guide.append("\n## Hotspots\n")
        # QA hotspots
        qa_hot = [it for it in (qa_eval.items or []) if (it.correctness_score or 0) < low_threshold or (it.verdict or '').lower() in {"incorrect", "unclear"}]
        qa_hot = sorted(qa_hot, key=lambda x: (x.correctness_score, x.timestamp))[:max_qa_hotspots]
        if qa_hot:
            guide.append("### –°–ª–∞–±—ã–µ –æ—Ç–≤–µ—Ç—ã (Q&A)\n")
            for it in qa_hot:
                guide.append(f"- [{it.timestamp}] {it.topic} ({it.difficulty}) ‚Äî {it.correctness_score}/10 ({it.verdict})\n  –í–æ–ø—Ä–æ—Å: {it.question}\n  –ß—Ç–æ –æ—Ç–≤–µ—Ç–∏–ª: {it.answer_summary}\n  –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {it.ideal_answer}\n  –ü–æ–¥—Ç—è–Ω—É—Ç—å: {', '.join(it.what_to_improve or [])}\n  –ß–µ–≥–æ –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ: {', '.join(it.missing_points or [])}\n")
        # Mistakes hotspots
        high_mistakes = [m for m in (mistakes_analysis.issues or []) if (m.severity or '').lower() == 'high']
        if high_mistakes:
            guide.append("\n### –ö—Ä–∏—Ç–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏\n")
            for m in high_mistakes:
                guide.append(f"- [{m.timestamp}] {m.type} ‚Äî {m.explanation}\n  –§—Ä–∞–∑–∞: \"{m.quote}\"\n  –ü—Ä–∞–≤–∏–ª—å–Ω–æ: {m.correction}\n  –¢–µ–≥–∏: {', '.join(m.tags or [])}\n")

        # –¢–µ–º—ã –¥–ª—è –ø–æ–¥—Ç—è–∂–∫–∏ –ø–æ —Å—Ä–µ–¥–Ω–∏–º –±–∞–ª–ª–∞–º
        guide.append("\n## –¢–µ–º—ã –¥–ª—è –ø–æ–¥—Ç—è–∂–∫–∏\n")
        topics = list(getattr(qa_eval.summary, 'by_topic', []) or [])
        try:
            topics_sorted = sorted(topics, key=lambda t: float(t.get('avg_score', 0)))
        except Exception:
            topics_sorted = topics
        worst_topics = topics_sorted[:3] if topics_sorted else []
        if worst_topics:
            for t in worst_topics:
                topic = t.get('topic', 'other')
                avg = t.get('avg_score', 0)
                guide.append(f"### {topic}: —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª {avg}/10\n")
                # –°–æ–±–µ—Ä—ë–º —á–µ–∫–ª–∏—Å—Ç –ø–æ —Ç–µ–º–µ –∏–∑ Q&A
                checklist = []
                for it in (qa_eval.items or []):
                    if it.topic == topic:
                        checklist.extend(it.what_to_improve or [])
                        checklist.extend(it.missing_points or [])
                # –î–µ–¥—É–ø –∏ –æ–±—Ä–µ–∑–∫–∞
                seen = set()
                uniq = []
                for x in checklist:
                    x = x.strip()
                    if not x or x.lower() in seen:
                        continue
                    seen.add(x.lower())
                    uniq.append(x)
                if uniq:
                    guide.append("–ß–µ–∫–ª–∏—Å—Ç:\n")
                    for i, item in enumerate(uniq[:10], 1):
                        guide.append(f"- [ ] {item}\n")
                guide.append("\n")

        # –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∏ –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏–∑ —Å–≤–æ–¥–∫–∏
        if getattr(qa_eval.summary, 'top_strengths', None):
            guide.append("## –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã\n")
            for x in qa_eval.summary.top_strengths:
                guide.append(f"- {x}\n")
            guide.append("\n")
        if getattr(qa_eval.summary, 'key_gaps', None):
            guide.append("## –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–µ–ª—ã\n")
            for x in qa_eval.summary.key_gaps:
                guide.append(f"- {x}\n")
            guide.append("\n")

        return "".join(guide)

    # –ü–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ config.json/yaml (CLI –µ–≥–æ –ø—Ä–æ–∫–∏–¥—ã–≤–∞–µ—Ç –≤ –≤—ã—Ö–æ–¥–Ω–æ–π JSON, –∞ –∑–¥–µ—Å—å –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç)
    cfg = load_config(None)
    low_thr = float(cfg.get("qa_low_threshold", 6.0))
    max_hot = int((cfg.get("study_guide") or {}).get("max_qa_hotspots", 20))
    study_md = _build_study_guide(low_threshold=low_thr, max_qa_hotspots=max_hot)
    study_file = out_dir / f"{stem}_study_guide.md"
    with open(study_file, 'w', encoding='utf-8') as f:
        f.write(study_md)


    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üö© –û—à–∏–±–∫–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {mistakes_output_file}")
    print(f"üßæ QA-–æ—Ü–µ–Ω–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {qa_output_file}")
    print(f"üß† –ü–ª–∞–Ω –ø—Ä–æ–∫–∞—á–∫–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {study_file}")

    # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –≤ –∫–æ–Ω—Å–æ–ª—å –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –æ—à–∏–±–æ–∫
    by_sev = {"high": 0, "medium": 0, "low": 0}
    for it in (mistakes_analysis.issues or []):
        sev = (it.severity or '').lower()
        if sev not in by_sev:
            by_sev[sev] = 0
        by_sev[sev] += 1

    print("\n" + "=" * 50)
    print("–ö–†–ê–¢–ö–ò–ô –û–¢–ß–ï–¢ –ü–û –û–®–ò–ë–ö–ê–ú")
    print("=" * 50)
    print(f"High: {by_sev.get('high', 0)} | Medium: {by_sev.get('medium', 0)} | Low: {by_sev.get('low', 0)}")
    print(f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(mistakes_analysis.issues or [])}")

    # –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –ø–æ QA
    print("\n" + "=" * 50)
    print("–ö–†–ê–¢–ö–ê–Ø QA-–°–í–û–î–ö–ê")
    print("=" * 50)
    print(f"–í–æ–ø—Ä–æ—Å–æ–≤: {qa_eval.summary.total_questions} | –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏: {qa_eval.summary.average_score:.1f}/10")
    if qa_eval.summary.by_topic:
        try:
            sorted_topics = sorted(qa_eval.summary.by_topic, key=lambda t: float(t.get('avg_score', 0)))
            worst = sorted_topics[:3]
            if worst:
                print("–¢–µ–º—ã –¥–ª—è –ø–æ–¥—Ç—è–∂–∫–∏:")
                for t in worst:
                    print(f"- {t.get('topic')}: {t.get('avg_score')}/10")
        except Exception:
            pass

    # –í–µ—Ä–Ω—ë–º –æ–±—ä–µ–∫—Ç —Å –æ—à–∏–±–∫–∞–º–∏ –∏ –¥–æ–±–∞–≤–∏–º QA-–æ—Ü–µ–Ω–∫—É –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π —Å–≤–æ–¥–∫–∏ –≤ CLI
    try:
        setattr(mistakes_analysis, 'qa_eval', qa_eval)
    except Exception:
        pass
    return mistakes_analysis
