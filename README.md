# Tech Interview Analyzer

Инструмент для автоматического разбора записей технических интервью. Проект использует WhisperX для точной транскрибации с диаризацией и Google Gemini для поиска ошибок в ответах кандидата, оценки Q&A и формирования плана дальнейшего развития.

## Возможности
- Транскрибация аудио и видео через WhisperX с распознаванием спикеров и кешированием результата.
- Выявление ошибок кандидата (severity, тип, цитата, корректировка) на основе инструкций для LLM.
- Оценка ответов по каждому вопросу интервьюера: тема, сложность, итоговый балл, рекомендации.
- Автоматическое формирование отчета по ошибкам, QA-ревью и учебного плана (hotspots, темы для роста).
- CLI для пакетной обработки файлов и сводки по нескольким сессиям.
- Гибкая настройка моделей, чанков и порогов качества через config.yaml/json.

## Архитектура проекта
- `main.py` — основной сценарий анализа: транскрибация, загрузка кеша, запуск LLM-анализаторов, сохранение результатов.
- `cli.py` — интерфейс командной строки с поддержкой glob-паттернов, агрегированной статистикой и кастомным output-dir.
- `analizers/mistake_analyzer.py` — поиск ошибок в ответах кандидата (Issue, MistakesAnalysis).
- `analizers/qa_evaluator.py` — извлечение Q&A, оценка ответов, генерация сводки по темам.
- `asr.py` — асинхронная транскрибация WhisperX с выравниванием и диаризацией.
- `config.py` — загрузка дефолтных параметров и их объединение с YAML/JSON конфигом.
- `results/` — папка с готовыми отчетами; структура формируется на лету.

## Требования
- Python 3.11 или 3.12 (см. `pyproject.toml`).
- Poetry 1.6+ (для управления зависимостями) либо любой другой менеджер виртуальных окружений.
- FFmpeg установленный в системе (`ffmpeg -version`) — обязателен для WhisperX.
- Достаточно памяти (WhisperX large-v2 > 10 GB) и желательно наличие GPU, хотя в `asr.py` по умолчанию выбран CPU.
- Аккаунт в Google AI Studio и API-ключ Google Generative AI.
- Hugging Face токен с доступом к моделям диаризации (pyannote).

## Установка
```bash
# Клонируйте репозиторий
 git clone https://github.com/Qu1ck1337/tech-interview-analizer.git
 cd tech-interview-analizer

# Установите зависимости (вариант с Poetry)
 poetry install
```

### Настройка окружения
1. Скопируйте шаблон переменных:
   ```bash
   cp env_example.txt .env
   ```
2. Заполните `.env` или экспортируйте вручную:
   - `GOOGLE_API_KEY` — ключ Google Gemini (https://makersuite.google.com/app/apikey)
   - `HF_TOKEN` — токен Hugging Face с доступом к `pyannote.audio`
3. При первом запуске WhisperX скачает модели; убедитесь, что есть место на диске.

## Конфигурация
Файл `config.yaml` (или `config.json`) переопределяет дефолтные значения из `config.py`. Основные параметры:

| Ключ | Описание |
| ---- | -------- |
| `output_dir` | Папка, куда складываются все итоговые файлы (`results` по умолчанию). |
| `qa_low_threshold` | Минимальный балл, ниже которого ответ считается слабым и попадает в раздел «Hotspots». |
| `study_guide.max_qa_hotspots` | Сколько слабых ответов показать в учебном плане. |
| `models.mistakes` / `models.qa` | Названия моделей Gemini для поиска ошибок и оценки Q&A. |
| `chunking.*` | Настройки нарезки записи на части, чтобы обрабатывать длинные видео небольшими кусками. |

Hotspots — это раздел в файле `<stem>_study_guide.md`, где собираются ответы с низкими баллами и короткие рекомендации по ним.

Можно передать альтернативный конфиг через `--config path/to/custom.yaml`.

### Пример конфигурации
```yaml
output_dir: reports                 # директория для файлов отчётов
qa_low_threshold: 7.0               # ответы ниже 7 баллов попадут в раздел «Hotspots»
models:                             # выбор моделей Gemini
  mistakes: gemini-2.5-flash        # модель для поиска ошибок в ответах
  qa: gemini-2.0-pro                # модель для оценки вопросов/ответов
chunking:                           # параметры нарезки записи на куски
  mistakes:
    enabled: true                   # включить нарезку записи для анализа ошибок
    chunk_duration_sec: 300         # длина одного куска в секундах
    overlap_sec: 30                 # перекрытие между кусками
    max_chunks: 6                   # максимум кусков, которые отправим в модель
    max_issues_hint: 12             # попросить модель ограничиться 12 ошибками
  qa:
    enabled: true                   # включить нарезку для Q&A анализа
    chunk_duration_sec: 360         # длина куска для Q&A
    overlap_sec: 30                 # перекрытие для Q&A кусочков
study_guide:                        # параметры учебного плана
  max_qa_hotspots: 15               # показать до 15 слабых ответов в учебном плане
```
Этот пример сохраняет отчёты в папку `reports`, считает ответы с баллом ниже 7 проблемными, использует альтернативные модели Gemini и делит запись на короткие куски. Остальные параметры берутся из словаря `DEFAULT_CONFIG` в `config.py`.

## Запуск
### CLI
```bash
# Базовый анализ одного видео
python cli.py <file_path>

# Анализ нескольких файлов + иной каталог результатов
python cli.py data/*.mp4 -o reports

# Использование кастомного конфига и VERBOSE-режима
python cli.py interviews/ -c configs/fast.yaml -v
```

Поддерживаемые расширения: `.mov`, `.mp4`, `.avi`, `.mkv`, `.wav`, `.mp3`, `.m4a`, `.flac`, а также `.json` с готовыми транскриптами. CLI автоматически перебирает файлы по glob-паттернам и внутри директорий.

Флаг `-v/--verbose` включает расширенный лог: показывается прогресс по каждому файлу, промежуточные подсчёты ошибок и подробные трассировки при исключениях. Без флага вывод остаётся сжатым.

### Программное использование
```python
import asyncio
from main import analyze_interview_file

async def run():
    report = await analyze_interview_file("interview.mov", output_dir="results")
    print(report.issues[0].explanation)

asyncio.run(run())
```
`analyze_interview_file` принимает готовые `segments` (если вы уже транскрибировали файл) и кеширует транскрипции в `results/<stem>/<stem>_transcript.json`.

## Структура результатов
После успешного анализа для каждого файла создается папка `results/<имя_файла>/` с артефактами:
```
<stem>_transcript.json  # транскрипция с таймкодами и спикерами
<stem>_mistakes.md      # список найденных ошибок и неточностей
<stem>_qa_review.md     # разметка вопросов, оценка ответов, рекомендации
<stem>_study_guide.md   # hotspots, тематики для подтяжки, чек-листы
```
Также CLI выводит суммарную статистику по ошибкам и средний балл по всем сессиям.

## Примеры отчетов
### Фрагмент `*_mistakes.md`
```markdown
# Ошибки кандидата
- [12:34] factual_error (high)
  Цитата: "Система всегда обрабатывает запросы последовательно"
  Что не так: В архитектуре используется пул воркеров, поэтому ответы идут параллельно.
  Как правильно: Указать, что очереди обрабатываются конкурентно, и описать механику балансировки.
```

### Фрагмент `*_qa_review.md`
```markdown
## Вопросы и ответы
- [08:15] system_design / medium — 5.5/10 (partially_correct)
  Вопрос: Как спроектировать кеш для API с 100k RPS?
  Ответ кандидата: "Поднимем Redis и будем обнулять TTL при обращении".
  Что улучшить: подумать о шардировании, резервных узлах и промахах кеша.
  Эталон: Redis Cluster, TTL, бекграунд инвалидация, circuit breaker.
```

### Консольная сводка CLI
```text
🎯 Найдено 2 файлов для анализа:
  - dataset/latest/GRI interview.mov
  - dataset/latest/RoRe group 2 interview.mov
📁 Базовая папка результатов: results
...
📊 ИТОГОВАЯ СВОДКА ПО НЕСКОЛЬКИМ СЕССИЯМ
✅ Успешно проанализировано: 2/2 файлов
🚩 Ошибки суммарно: 14 (High: 3, Medium: 7, Low: 4)
❓ QA: вопросов: 26, средний балл: 6.8/10, hotspots: 9 (порог 6.0)
```

## Производительность и советы
- WhisperX large-v2 тяжелый; при ограниченных ресурсах можно изменить модель в `asr.transcribe` или выполнять транскрибацию заранее.
- Для повторного анализа можно положить готовый `<stem>_transcript.json` в ту же директорию или указать `--output-dir` — транскрибация будет пропущена.
- При большом интервью включите чанки через `chunking.mistakes.enabled` и `chunking.qa.enabled`, чтобы распараллелить работу LLM.

## Дополнительная информация
- Примеры записей и результатов лежат в `dataset/` и `results/` (если они присутствуют в репозитории).
- Проект легко адаптировать под другие языки: WhisperX поддерживает много языков, а выбор модели Gemini зависит от качества инструкций.
- Для отладки используйте ключ `-v`, чтобы видеть промежуточные сводки и стеки ошибок.
